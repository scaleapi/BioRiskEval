scope: perf
time_limit: 1800
key_segments:
  # Modify keys to be renamed (str) or excluded (False) from run identifier. By default, all args under script_args are included.
  data_path: False
  clip_grad: False
  lr: False
  min_lr: False
  wu_steps: False
script_args:
  # All arguments referenced in the script string must be specified here.
  # Arguments not referenced in the script string must have the 'arg' field specified.
  # See jet/core/configs.py for the specification of the configuration class
  workspace: /workspace/bionemo2
  data_path: /data/evo2
  model: evo2
  variant: train
  config_name: 1b
  precision: fp8
  gpus: 8
  batch_size: 8
  max_steps: 490000
  stop_steps: 600
  cp: 1
  seq_len: 8192
  acc_grad: 1
  clip_grad: 250
  seed: 3735928559
  lr: 0.00015
  min_lr: 0.000015
  wu_steps: 5000
  wd: 0.1
  products:
    - nodes: 2
      pp: 1
      tp: 1
    - nodes: 1
      pp: 1
      tp: 1
    # FIXME, issue: https://github.com/NVIDIA/bionemo-framework/issues/814
    # - nodes: 2
    #   pp: 1
    #   tp: 2
    # FIXME, issue: https://github.com/NVIDIA/bionemo-framework/issues/815
    # - nodes: 2
    #   pp: 2
    #   tp: 1
script: |-
  WANDB_API_KEY=$BIONEMO_WANDB_API_KEY ${variant}_${model} \
  -d /workspace/bionemo2/sub-packages/bionemo-evo2/examples/configs/full_pretrain_shortphase_config.yaml \
  --dataset-dir ${data_path} \
  --grad-acc-batches ${acc_grad} \
  --fp8 --fp8-wgrad --activation-checkpoint-recompute-num-layers 5 \
  --enable-preemption \
  --ckpt-async-save \
  --use-megatron-comm-overlap-llama3-8k \
  --overlap-grad-reduce \
  --clip-grad=${clip_grad} \
  --eod-pad-in-loss-mask \
  --seq-length=${seq_len} \
  --seed ${seed} \
  --lr=${lr} \
  --wd=${wd} \
  --min-lr=${min_lr} \
  --warmup-steps=${wu_steps} \
  --tensor-parallel-size=${tp} \
  --context-parallel-size=${cp} \
  --pipeline-model-parallel-size=${pp} \
  --workers 8 \
  --num-nodes=${nodes} \
  --devices=${gpus} \
  --micro-batch-size=${batch_size} \
  --model-size=${config_name} \
  --max-steps=${max_steps} \
  --early-stop-on-step ${stop_steps} \
  --limit-val-batches=20 \
  --log-every-n-steps=50 \
  --val-check-interval=200 \
  --create-tflops-callback \
  --create-tensorboard-logger \
  --result-dir=${tensorboard_dir} \
  --wandb-project=${wandb_project_name} \
  --wandb-group=${model}_${variant}_${config_name}__${target}__slen${seq_len} \
  --wandb-job-type=${pipeline_label} \
  --wandb-run-name=${batch_size}bs_${nodes}node_${gpus}gpu_${max_steps}s_${precision}prec_tp${tp}_pp_${pp} \
  --disable-checkpointing;
